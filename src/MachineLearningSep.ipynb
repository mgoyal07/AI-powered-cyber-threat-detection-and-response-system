{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "B4m7YCKWpPTv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4m7YCKWpPTv",
        "outputId": "e92fba9f-9970-401a-d1bc-1f4fcabfc852"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "98491fd3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98491fd3",
        "outputId": "e9aeb6d8-67d6-4121-e6bb-09b44df04698"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting cudatree\n",
            "  Downloading cudatree-0.6.tar.gz (22 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "pip install cudatree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4bf17ae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bf17ae3",
        "outputId": "ee109cfc-62ad-42bd-9f89-1c0962718b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 582, done.\u001b[K\n",
            "remote: Counting objects: 100% (148/148), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 582 (delta 119), reused 82 (delta 82), pack-reused 434 (from 3)\u001b[K\n",
            "Receiving objects: 100% (582/582), 190.86 KiB | 4.06 MiB/s, done.\n",
            "Resolving deltas: 100% (293/293), done.\n",
            "Using Python 3.11.11 environment at: /usr\n",
            "Resolved 2 packages in 137ms\n",
            "Prepared 2 packages in 24ms\n",
            "Installed 2 packages in 3ms\n",
            " + nvidia-ml-py==12.570.86\n",
            " + pynvml==12.0.0\n",
            "Installing RAPIDS remaining 24.12.* libraries\n",
            "Using Python 3.11.11 environment at: /usr\n",
            "Resolved 154 packages in 1.09s\n",
            "Downloading cuml-cu12 (522.6MiB)\n",
            "Downloading libcuspatial-cu12 (16.9MiB)\n",
            "Downloading dask (1.2MiB)\n",
            "Downloading libucx-cu12 (25.7MiB)\n",
            "Downloading datashader (17.5MiB)\n",
            "Downloading scikit-image (14.2MiB)\n",
            "Downloading cugraph-cu12 (877.5MiB)\n",
            "Downloading raft-dask-cu12 (187.8MiB)\n",
            "Downloading cucim-cu12 (5.3MiB)\n",
            "Downloading cuvs-cu12 (810.3MiB)\n",
            "Downloading cuspatial-cu12 (4.8MiB)\n",
            "Downloading ucx-py-cu12 (2.2MiB)\n",
            " Downloaded ucx-py-cu12\n",
            " Downloaded cuspatial-cu12\n",
            " Downloaded libcuspatial-cu12\n",
            " Downloaded libucx-cu12\n",
            " Downloaded datashader\n",
            " Downloaded dask\n",
            " Downloaded cucim-cu12\n",
            " Downloaded scikit-image\n",
            " Downloaded raft-dask-cu12\n",
            " Downloaded cuml-cu12\n",
            " Downloaded cugraph-cu12\n",
            " Downloaded cuvs-cu12\n",
            "Prepared 30 packages in 49.49s\n",
            "Uninstalled 3 packages in 55ms\n",
            "Installed 30 packages in 155ms\n",
            " + cucim-cu12==24.12.0\n",
            " + cugraph-cu12==24.12.0\n",
            " + cuml-cu12==24.12.0\n",
            " + cuproj-cu12==24.12.0\n",
            " + cuspatial-cu12==24.12.0\n",
            " + cuvs-cu12==24.12.0\n",
            " + cuxfilter-cu12==24.12.0\n",
            " - dask==2024.10.0\n",
            " + dask==2024.11.2\n",
            " + dask-cuda==24.12.0\n",
            " + dask-cudf-cu12==24.12.0\n",
            " + dask-expr==1.1.19\n",
            " + datashader==0.17.0\n",
            " + distributed==2024.11.2\n",
            " + distributed-ucxx-cu12==0.41.0\n",
            " + jupyter-server-proxy==4.4.0\n",
            " + libcuspatial-cu12==24.12.0\n",
            " + libucx-cu12==1.17.0.post1\n",
            " + libucxx-cu12==0.41.0\n",
            " + pyct==0.5.0\n",
            " - pynvml==12.0.0\n",
            " + pynvml==11.4.1\n",
            " + raft-dask-cu12==24.12.0\n",
            " + rapids-dask-dependency==24.12.0\n",
            " - scikit-image==0.25.2\n",
            " + scikit-image==0.24.0\n",
            " + simpervisor==1.0.0\n",
            " + sortedcontainers==2.4.0\n",
            " + tblib==3.0.0\n",
            " + treelite==4.3.0\n",
            " + ucx-py-cu12==0.41.0\n",
            " + ucxx-cu12==0.41.0\n",
            " + zict==3.0.0\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "# Colab warns and provides remediation steps if the GPUs is not compatible with RAPIDS.\n",
        "\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oojel6Nfxcb4",
      "metadata": {
        "id": "oojel6Nfxcb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e2M5rHEf2NK9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2M5rHEf2NK9",
        "outputId": "336ea3c9-6ca5-4786-b180-c97317a1b8a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Attack Type  Naive Bayes (GPU)  QDA (CPU)  MLP (GPU, PyTorch)\n",
            "0               Bot           0.638021   0.998870            0.998860\n",
            "1              DDoS           0.987697   0.979342            0.884113\n",
            "2     DoS GoldenEye           0.865831   0.984525            0.006007\n",
            "3          DoS Hulk           0.725127   0.875710            0.729039\n",
            "4  DoS Slowhttptest           0.853011   0.975157            0.911235\n",
            "5     DoS slowloris           0.866002   0.980956            0.996625\n",
            "6       FTP-Patator           0.004545   0.998776            0.184951\n",
            "7          PortScan           0.916335   0.916335            0.916335\n",
            "8       SSH-Patator           0.996145   0.998788            0.996560\n"
          ]
        }
      ],
      "source": [
        "import cudf\n",
        "import cupy as cp\n",
        "from cuml.naive_bayes import GaussianNB as cuGaussianNB\n",
        "\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "#######################################\n",
        "# 1) PyTorch MLP Definition (GPU)\n",
        "#######################################\n",
        "class SimpleMLP(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic MLP with one hidden layer of size hidden_dim,\n",
        "    ReLU activation, output_dim=2 for binary classification.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=32, output_dim=2):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x  # shape (N, 2) for 2-class output\n",
        "\n",
        "def train_mlp_pytorch(X_train, y_train, X_test, y_test, epochs=5, lr=0.001):\n",
        "    \"\"\"\n",
        "    Trains a simple MLP in PyTorch on GPU (if available).\n",
        "    Returns the accuracy on the test set.\n",
        "    Inputs/Outputs should be NumPy arrays (float32 for X, int for y).\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Convert data to PyTorch tensors and move to GPU/CPU\n",
        "    X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "    y_train_t = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "\n",
        "    X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    y_test_t = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "    # Define the model\n",
        "    input_dim = X_train.shape[1]\n",
        "    model = SimpleMLP(input_dim=input_dim, hidden_dim=32, output_dim=2).to(device)\n",
        "\n",
        "    # Loss & Optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Training Loop (simple: all data in one batch)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train_t)\n",
        "        loss = criterion(outputs, y_train_t)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Optional: print the training loss\n",
        "        # print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test_t)               # shape (N, 2)\n",
        "        _, predicted = torch.max(logits, 1)    # shape (N,)\n",
        "        correct = (predicted == y_test_t).sum().item()\n",
        "        accuracy = correct / float(y_test_t.shape[0])\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "########################################\n",
        "# 2) Main Pipeline for Attack Types\n",
        "########################################\n",
        "attack_types = [\n",
        "    \"Bot\",\n",
        "    \"DDoS\",\n",
        "    \"DoS GoldenEye\",\n",
        "    \"DoS Hulk\",\n",
        "    \"DoS Slowhttptest\",\n",
        "    \"DoS slowloris\",\n",
        "    \"FTP-Patator\",\n",
        "    \"PortScan\",\n",
        "    \"SSH-Patator\"\n",
        "]\n",
        "benign_type = \"BENIGN\"\n",
        "\n",
        "results = []\n",
        "\n",
        "for attack_type in attack_types:\n",
        "    # --------------------------\n",
        "    # A) Read top features\n",
        "    # --------------------------\n",
        "    importance_file = (\n",
        "        f\"/content/drive/MyDrive/Threat-Detection-in-Cyber-Security-Using-AI-master/\"\n",
        "        f\"{attack_type}_importance.csv\"\n",
        "    )\n",
        "    importance_data = pd.read_csv(importance_file)\n",
        "    selected_features = importance_data[\"Feature\"][:3].tolist()\n",
        "\n",
        "    # --------------------------\n",
        "    # B) Load data on GPU, label -> 0/1\n",
        "    # --------------------------\n",
        "    data_file = (\n",
        "        f\"/content/drive/MyDrive/Threat-Detection-in-Cyber-Security-Using-AI-master/\"\n",
        "        f\"{attack_type}_vs_{benign_type}.csv\"\n",
        "    )\n",
        "    df_gpu = cudf.read_csv(data_file)\n",
        "\n",
        "    # Convert \" Label\" to 0/1\n",
        "    df_gpu[\" Label\"] = (df_gpu[\" Label\"] != \"BENIGN\").astype(\"int32\")\n",
        "\n",
        "    # Subset the features\n",
        "    X_gpu = df_gpu[selected_features].astype(\"float32\")\n",
        "    y_gpu = df_gpu[\" Label\"]  # int32\n",
        "\n",
        "    # Convert to Pandas for train_test_split\n",
        "    X_cpu = X_gpu.to_pandas()\n",
        "    y_cpu = y_gpu.to_pandas()\n",
        "\n",
        "    # --------------------------\n",
        "    # C) Train/Test Split (CPU)\n",
        "    # --------------------------\n",
        "    X_train_cpu, X_test_cpu, y_train_cpu, y_test_cpu = train_test_split(\n",
        "        X_cpu, y_cpu, test_size=0.4, random_state=42\n",
        "    )\n",
        "\n",
        "    # --------------------------\n",
        "    # D) GPU-based Naive Bayes\n",
        "    # --------------------------\n",
        "    # Convert splits to cuDF -> Cupy\n",
        "    X_train_gpu = cudf.DataFrame.from_pandas(X_train_cpu)\n",
        "    X_test_gpu  = cudf.DataFrame.from_pandas(X_test_cpu)\n",
        "    y_train_gpu = cudf.Series(y_train_cpu)\n",
        "    y_test_gpu  = cudf.Series(y_test_cpu)\n",
        "\n",
        "    X_train_cp = X_train_gpu.to_cupy()\n",
        "    y_train_cp = y_train_gpu.to_cupy()\n",
        "    X_test_cp  = X_test_gpu.to_cupy()\n",
        "    y_test_cp  = y_test_gpu.to_cupy()\n",
        "\n",
        "    nb_model_gpu = cuGaussianNB()\n",
        "    nb_model_gpu.fit(X_train_cp, y_train_cp)\n",
        "\n",
        "    nb_preds_cp  = nb_model_gpu.predict(X_test_cp)\n",
        "    nb_preds_cpu = cp.asnumpy(nb_preds_cp)\n",
        "    y_test_cpu_nb = cp.asnumpy(y_test_cp)\n",
        "\n",
        "    nb_accuracy = accuracy_score(y_test_cpu_nb, nb_preds_cpu)\n",
        "\n",
        "    # --------------------------\n",
        "    # E) CPU-based QDA\n",
        "    # --------------------------\n",
        "    qda = QuadraticDiscriminantAnalysis()\n",
        "    qda.fit(X_train_cpu, y_train_cpu)\n",
        "    qda_preds = qda.predict(X_test_cpu)\n",
        "    qda_accuracy = accuracy_score(y_test_cpu, qda_preds)\n",
        "\n",
        "    # --------------------------\n",
        "    # F) GPU-based MLP (PyTorch)\n",
        "    # --------------------------\n",
        "    # Our function expects NumPy arrays (float32 for X, int for y).\n",
        "    # They are already in CPU memory after train_test_split\n",
        "    X_train_np = X_train_cpu.values.astype(\"float32\")\n",
        "    y_train_np = y_train_cpu.values.astype(\"int64\")  # for PyTorch cross-entropy\n",
        "    X_test_np  = X_test_cpu.values.astype(\"float32\")\n",
        "    y_test_np  = y_test_cpu.values.astype(\"int64\")\n",
        "\n",
        "    mlp_accuracy = train_mlp_pytorch(X_train_np, y_train_np, X_test_np, y_test_np,\n",
        "                                     epochs=5, lr=0.001)\n",
        "\n",
        "    # --------------------------\n",
        "    # G) Store results\n",
        "    # --------------------------\n",
        "    results.append({\n",
        "        \"Attack Type\": attack_type,\n",
        "        \"Naive Bayes (GPU)\": nb_accuracy,\n",
        "        \"QDA (CPU)\": qda_accuracy,\n",
        "        \"MLP (GPU, PyTorch)\": mlp_accuracy\n",
        "    })\n",
        "\n",
        "# --------------------------------\n",
        "# 3) Build Results DataFrame\n",
        "# --------------------------------\n",
        "df_results = pd.DataFrame(results)\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AvwLeA2Ixcg-",
      "metadata": {
        "id": "AvwLeA2Ixcg-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "eJ-iNGKnxcja",
      "metadata": {
        "id": "eJ-iNGKnxcja"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 0 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n",
            "c:\\Users\\91701\\Thesis\\Threat-Detection-in-Cyber-Security-Using-AI-master\\.venv\\Lib\\site-packages\\sklearn\\discriminant_analysis.py:1024: LinAlgWarning: The covariance matrix of class 1 is not full rank. Increasing the value of parameter `reg_param` might help reducing the collinearity.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Attack Type  Naive Bayes Accuracy  QDA Accuracy  MLP Accuracy\n",
            "0               Bot              0.638021      0.832589      0.998870\n",
            "1              DDoS              0.984610      0.980481      0.992264\n",
            "2     DoS GoldenEye              0.984251      0.984238      0.996942\n",
            "3          DoS Hulk              0.977031      0.976215      0.989537\n",
            "4  DoS Slowhttptest              0.972274      0.974597      0.996782\n",
            "5     DoS slowloris              0.981212      0.978324      0.997528\n",
            "6       FTP-Patator              0.998758      0.998777      0.995455\n",
            "7          PortScan              0.916335      0.916335      0.998157\n",
            "8       SSH-Patator              0.998276      0.998788      0.998808\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from incident_handler import respond_to_threat  # ⬅️ make sure incident_handler.py is in the same folder\n",
        "\n",
        "# Simulate IPs just for demo\n",
        "import random\n",
        "\n",
        "# List of attack types\n",
        "attack_types = [\"Bot\", \"DDoS\", \"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\", \"DoS slowloris\", \"FTP-Patator\",\n",
        "                \"PortScan\", \"SSH-Patator\"]\n",
        "benign_type = \"BENIGN\"\n",
        "\n",
        "# Initialize an empty list to store results as dictionaries\n",
        "results = []\n",
        "\n",
        "# Loop through each attack type\n",
        "for attack_type in attack_types:\n",
        "    # Read the feature importance file\n",
        "    # importance_file = f\"{attack_type}_importance.csv\"\n",
        "    importance_file = (\n",
        "        f\"C:/Users/91701/Thesis/Threat-Detection-in-Cyber-Security-Using-AI-master/dataset/\"\n",
        "        f\"{attack_type}_importance.csv\"\n",
        "    )\n",
        "    importance_data = pd.read_csv(importance_file)\n",
        "\n",
        "    # Select the first 4 features\n",
        "    selected_features = importance_data['Feature'][:3].tolist()\n",
        "\n",
        "    # Read the data file\n",
        "    # data_file = f\"{attack_type}_vs_{benign_type}.csv\"\n",
        "    data_file = (\n",
        "        f\"C:/Users/91701/Thesis/Threat-Detection-in-Cyber-Security-Using-AI-master/dataset/\"\n",
        "        f\"{attack_type}_vs_{benign_type}.csv\"\n",
        "    )\n",
        "    data = pd.read_csv(data_file)\n",
        "\n",
        "    # Select the selected features and the target column\n",
        "    selected_data = data[selected_features + [' Label']]\n",
        "\n",
        "    # Split the data into features (X) and target (y)\n",
        "    X = selected_data[selected_features]\n",
        "    y = selected_data[' Label']\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "\n",
        "    # Train Naive Bayes\n",
        "    nb_model = GaussianNB()\n",
        "    nb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Train Quadratic Discriminant Analysis\n",
        "    qda_model = QuadraticDiscriminantAnalysis()\n",
        "    qda_model.fit(X_train, y_train)\n",
        "\n",
        "    # Train Multi-Layer Perceptron\n",
        "    mlp_model = MLPClassifier(random_state=42, max_iter=1000, learning_rate_init=0.001)\n",
        "    mlp_model.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    nb_preds = nb_model.predict(X_test)\n",
        "    qda_preds = qda_model.predict(X_test)\n",
        "    mlp_preds = mlp_model.predict(X_test)\n",
        "    mlp_probs = mlp_model.predict_proba(X_test)  # shape: [n_samples, 2]\n",
        "\n",
        "    for i in range(len(mlp_preds)):\n",
        "        if mlp_preds[i] == 1:  # 1 = attack\n",
        "            confidence = max(mlp_probs[i])  # the model’s certainty\n",
        "            random_ip = f\"192.168.1.{random.randint(1, 255)}\"  # fake IP for now\n",
        "            respond_to_threat(attack_type, random_ip, confidence)\n",
        "\n",
        "\n",
        "    # Calculate accuracies\n",
        "    nb_accuracy = accuracy_score(y_test, nb_preds)\n",
        "    qda_accuracy = accuracy_score(y_test, qda_preds)\n",
        "    mlp_accuracy = accuracy_score(y_test, mlp_preds)\n",
        "\n",
        "    # Store the results as a dictionary\n",
        "    result_dict = {\n",
        "        'Attack Type': attack_type,\n",
        "        'Naive Bayes Accuracy': nb_accuracy,\n",
        "        'QDA Accuracy': qda_accuracy,\n",
        "        'MLP Accuracy': mlp_accuracy\n",
        "    }\n",
        "\n",
        "    # Append the dictionary to the results list\n",
        "    results.append(result_dict)\n",
        "\n",
        "# Create a Pandas DataFrame from the results list\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "80d7ff08",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "46e959ca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Combined shape: (16230880, 79)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "attack_types = [\"Bot\", \"DDoS\", \"DoS GoldenEye\", \"DoS Hulk\", \"DoS Slowhttptest\",\n",
        "                \"DoS slowloris\", \"FTP-Patator\", \"PortScan\", \"SSH-Patator\"]\n",
        "benign_type = \"BENIGN\"\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for attack in attack_types:\n",
        "    path = f\"C:/Users/91701/Thesis/Threat-Detection-in-Cyber-Security-Using-AI-master/dataset/{attack}_vs_{benign_type}.csv\"\n",
        "    df = pd.read_csv(path)\n",
        "    df[\" Label\"] = df[\" Label\"].apply(lambda v: 0 if v.strip().upper() == \"BENIGN\" else 1)\n",
        "    all_data.append(df)\n",
        "\n",
        "df_all = pd.concat(all_data, ignore_index=True)\n",
        "print(\"✅ Combined shape:\", df_all.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b26fe53b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Saved combined dataset to: C:/Users/91701/Thesis/Threat-Detection-in-Cyber-Security-Using-AI-master/src\\df_all.csv\n"
          ]
        }
      ],
      "source": [
        "DATASET_DIR = \"C:/Users/91701/Thesis/Threat-Detection-in-Cyber-Security-Using-AI-master/dataset\"\n",
        "SAVE_DIR = \"C:/Users/91701/Thesis/Threat-Detection-in-Cyber-Security-Using-AI-master/src\"\n",
        "combined_csv_path = os.path.join(SAVE_DIR, \"df_all.csv\")\n",
        "df_all.to_csv(combined_csv_path, index=False)\n",
        "print(f\"💾 Saved combined dataset to: {combined_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c0a21dca",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Saved: mlp_model.pkl and scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "# ✅ Step 3: Select top features\n",
        "# 👉 You can update this list based on your actual importance scores\n",
        "selected_features = [\" Flow Duration\", \" Bwd Packet Length Mean\", \" Packet Length Std\"]\n",
        "\n",
        "if not all(f in df_all.columns for f in selected_features):\n",
        "    missing = [f for f in selected_features if f not in df_all.columns]\n",
        "    raise ValueError(f\"❌ Missing features in dataset: {missing}\")\n",
        "\n",
        "X = df_all[selected_features].astype(\"float32\")\n",
        "y = df_all[\" Label\"].astype(\"int\")\n",
        "\n",
        "# ✅ Step 4: Scale & train\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "mlp_model = MLPClassifier(random_state=42, max_iter=1000, learning_rate_init=0.001)\n",
        "mlp_model.fit(X_scaled, y)\n",
        "\n",
        "# ✅ Step 5: Save model and scaler\n",
        "joblib.dump(mlp_model, os.path.join(SAVE_DIR, \"mlp_model.pkl\"))\n",
        "joblib.dump(scaler, os.path.join(SAVE_DIR, \"scaler.pkl\"))\n",
        "print(\"✅ Saved: mlp_model.pkl and scaler.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "60e6ce64",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>...</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.401097</td>\n",
              "      <td>-0.379440</td>\n",
              "      <td>-0.010777</td>\n",
              "      <td>-0.009744</td>\n",
              "      <td>-0.064632</td>\n",
              "      <td>-0.008531</td>\n",
              "      <td>-0.235865</td>\n",
              "      <td>0.479238</td>\n",
              "      <td>-0.067529</td>\n",
              "      <td>-0.312913</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005074</td>\n",
              "      <td>-0.099053</td>\n",
              "      <td>-0.109234</td>\n",
              "      <td>-0.137630</td>\n",
              "      <td>-0.073297</td>\n",
              "      <td>-0.282251</td>\n",
              "      <td>-0.080798</td>\n",
              "      <td>-0.284355</td>\n",
              "      <td>-0.273412</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.930937</td>\n",
              "      <td>-0.326029</td>\n",
              "      <td>-0.056730</td>\n",
              "      <td>-0.046895</td>\n",
              "      <td>-0.004781</td>\n",
              "      <td>-0.032457</td>\n",
              "      <td>0.690723</td>\n",
              "      <td>-0.536980</td>\n",
              "      <td>1.506978</td>\n",
              "      <td>1.972851</td>\n",
              "      <td>...</td>\n",
              "      <td>1.078168</td>\n",
              "      <td>-0.118177</td>\n",
              "      <td>-0.109978</td>\n",
              "      <td>-0.143077</td>\n",
              "      <td>-0.086475</td>\n",
              "      <td>-0.244274</td>\n",
              "      <td>-0.073967</td>\n",
              "      <td>-0.245803</td>\n",
              "      <td>-0.236290</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.333292</td>\n",
              "      <td>-0.655631</td>\n",
              "      <td>-0.010123</td>\n",
              "      <td>-0.010387</td>\n",
              "      <td>-0.088198</td>\n",
              "      <td>-0.007591</td>\n",
              "      <td>-0.377384</td>\n",
              "      <td>-0.177020</td>\n",
              "      <td>-0.346490</td>\n",
              "      <td>-0.366976</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.068350</td>\n",
              "      <td>-0.131771</td>\n",
              "      <td>-0.100491</td>\n",
              "      <td>-0.148812</td>\n",
              "      <td>-0.104465</td>\n",
              "      <td>-0.580707</td>\n",
              "      <td>-0.105787</td>\n",
              "      <td>-0.585977</td>\n",
              "      <td>-0.571490</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.428196</td>\n",
              "      <td>-0.390415</td>\n",
              "      <td>-0.011691</td>\n",
              "      <td>-0.012064</td>\n",
              "      <td>-0.070710</td>\n",
              "      <td>-0.009201</td>\n",
              "      <td>-0.350847</td>\n",
              "      <td>-0.439754</td>\n",
              "      <td>-0.445006</td>\n",
              "      <td>-0.332946</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.879153</td>\n",
              "      <td>-0.120297</td>\n",
              "      <td>-0.126787</td>\n",
              "      <td>-0.165393</td>\n",
              "      <td>-0.089440</td>\n",
              "      <td>-0.284261</td>\n",
              "      <td>-0.075630</td>\n",
              "      <td>-0.285737</td>\n",
              "      <td>-0.276032</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.358097</td>\n",
              "      <td>-0.654460</td>\n",
              "      <td>-0.007449</td>\n",
              "      <td>-0.008357</td>\n",
              "      <td>-0.065506</td>\n",
              "      <td>-0.007532</td>\n",
              "      <td>-0.324424</td>\n",
              "      <td>0.449044</td>\n",
              "      <td>-0.143664</td>\n",
              "      <td>-0.366976</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.068350</td>\n",
              "      <td>-0.131771</td>\n",
              "      <td>-0.100491</td>\n",
              "      <td>-0.148812</td>\n",
              "      <td>-0.104465</td>\n",
              "      <td>-0.580707</td>\n",
              "      <td>-0.105787</td>\n",
              "      <td>-0.585977</td>\n",
              "      <td>-0.571490</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 79 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Destination Port   Flow Duration   Total Fwd Packets  \\\n",
              "0          -0.401097       -0.379440           -0.010777   \n",
              "1           1.930937       -0.326029           -0.056730   \n",
              "2          -0.333292       -0.655631           -0.010123   \n",
              "3          -0.428196       -0.390415           -0.011691   \n",
              "4          -0.358097       -0.654460           -0.007449   \n",
              "\n",
              "    Total Backward Packets  Total Length of Fwd Packets  \\\n",
              "0                -0.009744                    -0.064632   \n",
              "1                -0.046895                    -0.004781   \n",
              "2                -0.010387                    -0.088198   \n",
              "3                -0.012064                    -0.070710   \n",
              "4                -0.008357                    -0.065506   \n",
              "\n",
              "    Total Length of Bwd Packets   Fwd Packet Length Max  \\\n",
              "0                     -0.008531               -0.235865   \n",
              "1                     -0.032457                0.690723   \n",
              "2                     -0.007591               -0.377384   \n",
              "3                     -0.009201               -0.350847   \n",
              "4                     -0.007532               -0.324424   \n",
              "\n",
              "    Fwd Packet Length Min   Fwd Packet Length Mean   Fwd Packet Length Std  \\\n",
              "0                0.479238                -0.067529               -0.312913   \n",
              "1               -0.536980                 1.506978                1.972851   \n",
              "2               -0.177020                -0.346490               -0.366976   \n",
              "3               -0.439754                -0.445006               -0.332946   \n",
              "4                0.449044                -0.143664               -0.366976   \n",
              "\n",
              "   ...   min_seg_size_forward  Active Mean   Active Std   Active Max  \\\n",
              "0  ...               1.005074    -0.099053    -0.109234    -0.137630   \n",
              "1  ...               1.078168    -0.118177    -0.109978    -0.143077   \n",
              "2  ...              -1.068350    -0.131771    -0.100491    -0.148812   \n",
              "3  ...              -0.879153    -0.120297    -0.126787    -0.165393   \n",
              "4  ...              -1.068350    -0.131771    -0.100491    -0.148812   \n",
              "\n",
              "    Active Min  Idle Mean   Idle Std   Idle Max   Idle Min   Label  \n",
              "0    -0.073297  -0.282251  -0.080798  -0.284355  -0.273412       0  \n",
              "1    -0.086475  -0.244274  -0.073967  -0.245803  -0.236290       0  \n",
              "2    -0.104465  -0.580707  -0.105787  -0.585977  -0.571490       0  \n",
              "3    -0.089440  -0.284261  -0.075630  -0.285737  -0.276032       0  \n",
              "4    -0.104465  -0.580707  -0.105787  -0.585977  -0.571490       0  \n",
              "\n",
              "[5 rows x 79 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d72f77fb",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
              "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
              "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
              "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
              "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
              "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
              "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
              "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
              "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
              "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
              "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
              "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
              "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
              "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
              "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
              "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
              "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
              "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
              "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
              "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
              "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
              "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
              "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
              "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
              "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
              "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
              "       ' Label'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_all.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "626f14a4",
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Enrichment complete.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[43menrich_csv_with_geo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36menrich_csv_with_geo\u001b[39m\u001b[34m(csv_path)\u001b[39m\n\u001b[32m     31\u001b[39m             df.at[i, \u001b[33m'\u001b[39m\u001b[33mlongitude\u001b[39m\u001b[33m'\u001b[39m] = lon\n\u001b[32m     32\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mip\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlon\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m         time.sleep(\u001b[32m0.5\u001b[39m)  \u001b[38;5;66;03m# Prevent rate-limiting\u001b[39;00m\n\u001b[32m     35\u001b[39m df.to_csv(csv_path, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Enrichment complete.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "\n",
        "CSV_PATH = \"incidents/incident_log.csv\"\n",
        "API_URL = \"https://ipinfo.io/{ip}/json\"\n",
        "\n",
        "def get_location(ip):\n",
        "    try:\n",
        "        response = requests.get(API_URL.format(ip=ip), timeout=3)\n",
        "        data = response.json()\n",
        "        if 'loc' in data:\n",
        "            lat, lon = data['loc'].split(',')\n",
        "            return float(lat), float(lon)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed for {ip}: {e}\")\n",
        "    return None, None\n",
        "\n",
        "def enrich_csv_with_geo(csv_path):\n",
        "    df = pd.read_csv(csv_path, on_bad_lines=\"skip\")  # pandas >= 1.3.0\n",
        "    if 'latitude' not in df.columns:\n",
        "        df['latitude'] = None\n",
        "        df['longitude'] = None\n",
        "\n",
        "    for i, row in df.iterrows():\n",
        "        if pd.isna(row['latitude']) or pd.isna(row['longitude']):\n",
        "            ip = row['ip_address']\n",
        "            lat, lon = get_location(ip)\n",
        "            if lat and lon:\n",
        "                df.at[i, 'latitude'] = lat\n",
        "                df.at[i, 'longitude'] = lon\n",
        "                print(f\"[{i}] {ip} → ({lat}, {lon})\")\n",
        "            time.sleep(0.5)  # Prevent rate-limiting\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(\"✅ Enrichment complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    enrich_csv_with_geo(CSV_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa5326f0",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
